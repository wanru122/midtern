import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
from datetime import datetime, timedelta
import json
from snownlp import SnowNLP
import random
import time
import re
import logging

# è¨­ç½®æ—¥èªŒè¨˜éŒ„
logging.basicConfig(filename='error.log', level=logging.ERROR, format='%(asctime)s - %(message)s')

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
apikey = os.getenv("API_KEY")
if not apikey:
    raise ValueError("API_KEY æœªæ­£ç¢ºè¨­ç½®ï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆã€‚")

# å®šç¾©æ—¥æœŸç¯„åœ
today = datetime.today()
month_ago = today - timedelta(days=30)
default_from_date = month_ago.strftime('%Y-%m-%d')
default_to_date = today.strftime('%Y-%m-%d')

# å®šç¾©ç›¸é—œé—œéµå­—
synonyms = {
    "å¥åº·": ["å¥åº·", "å¥èº«", "å¥åº·ç”Ÿæ´»"],
    "ç–¾ç—…": ["ç–¾ç—…", "ç—…ç—‡", "ç—…æƒ…"],
    "é†«ç™‚": ["é†«ç™‚", "é†«å­¸", "é†«è­·"],
    "é†«ç”Ÿ": ["é†«ç”Ÿ", "é†«å¸«"],
    "é†«é™¢": ["é†«é™¢", "è¨ºæ‰€"],
    "ä¿å¥": ["ä¿å¥", "é¤Šç”Ÿ", "è­·ç†"],
    "å…ç–«": ["å…ç–«", "å…ç–«ç³»çµ±"],
    "ç–«è‹—": ["ç–«è‹—", "æ¥ç¨®", "å…ç–«"],
    "è—¥ç‰©": ["è—¥ç‰©", "è—¥å“", "è—¥æ•ˆ"],
    "æ²»ç™‚": ["æ²»ç™‚", "é†«æ²»", "ç™‚æ³•"],
    "é¤Šç”Ÿ": ["é¤Šç”Ÿ", "ä¿å¥"],
    "ç–«æƒ…": ["ç–«æƒ…", "å‚³æŸ“ç—…", "ç–«ç—…"],
    "å¿ƒç†": ["å¿ƒç†", "å¿ƒç†å¥åº·", "ç²¾ç¥"],
}

relevant_keywords = list(set(word for synonyms_list in synonyms.values() for word in synonyms_list))

# æŸ¥è©¢åƒæ•¸è¨­ç½®
query_params = {
    'q': " OR ".join(relevant_keywords),
    'from': default_from_date,
    'to': default_to_date,
}

# ä½¿ç”¨è€…è‡ªå®šç¾©é—œéµå­—
user_query = input(f"è«‹è¼¸å…¥é—œéµå­—ï¼ˆé è¨­: {query_params['q']}ï¼‰ï¼š") or query_params['q']
query_params['q'] = user_query

# API è«‹æ±‚åƒæ•¸
params = {
    'q': query_params['q'],
    'language': 'zh',
    'from': query_params['from'],
    'to': query_params['to'],
    'apiKey': apikey,
    'sortBy': 'popularity',
    'pageSize': 100,
    'page': 1
}

# ç”¨ä¾†å„²å­˜ API æŠ“å–çš„æ–‡ç« 
api_articles = []

# API æŠ“å–å‡½æ•¸
def fetch_api_articles():
    global api_articles
    for page in range(1, 2):  # åªè«‹æ±‚ç¬¬ 1 é ï¼Œé¿å…è¶…éé™åˆ¶
        params['page'] = page
        try:
            response = requests.get('https://newsapi.org/v2/everything', params=params)
            response.raise_for_status()
            articles = response.json().get('articles', [])
            api_articles.extend(articles)
            if not articles:
                break
        except requests.exceptions.RequestException as e:
            logging.error(f"API ç¬¬ {page} é è«‹æ±‚å¤±æ•—: {e}")
            break

# çˆ¬å–å¥åº·é†«ç™‚ç¶²
def scrape_healthnews():
    articles = []
    try:
        url = "https://www.healthnews.com.tw/"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        for article in soup.find_all('a'):
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link or "javascript:void(0)" in link:
                continue
            if not link.startswith("http"):
                link = url.rstrip("/") + link
            if any(keyword in title for keyword in relevant_keywords):
                articles.append({
                    'title': title,
                    'url': link,
                    'source': "å¥åº·é†«ç™‚ç¶²",
                    'image': "https://via.placeholder.com/300"
                })
    except requests.exceptions.RequestException as e:
        logging.error(f"çˆ¬å–å¥åº·é†«ç™‚ç¶²å¤±æ•—: {e}")
    return articles

# çˆ¬å– Yahoo å¥åº·å°ˆå€
def scrape_yahoo_health():
    articles = []
    try:
        url = "https://tw.news.yahoo.com/health"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, allow_redirects=True)

        if "consent.yahoo.com" in response.url:
            logging.warning("è¢«é‡å®šå‘è‡³åŒæ„é é¢ï¼Œè·³éæ­¤é é¢ã€‚")
            return articles

        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        for article in soup.find_all('a'):
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link:
                continue
            if not link.startswith("http"):
                link = "https://tw.news.yahoo.com" + link
            if any(keyword in title for keyword in relevant_keywords):
                articles.append({
                    'title': title,
                    'url': link,
                    'source': "Yahoo å¥åº·å°ˆå€",
                    'image': "https://via.placeholder.com/300"
                })
    except requests.exceptions.RequestException as e:
        logging.error(f"çˆ¬å– Yahoo å¥åº·å°ˆå€å¤±æ•—: {e}")
    return articles

# æ•´åˆæ–‡ç« 
def integrate_articles(api_articles, scraped_articles):
    all_articles = []

    for article in api_articles:
        title = article.get('title', 'ç„¡æ¨™é¡Œ')
        description = article.get('description', 'ç„¡æè¿°')
        content = article.get('content', '')
        url = article.get('url', '#')
        image = article.get('urlToImage', 'https://via.placeholder.com/300')

        if any(keyword in title or keyword in description or keyword in content for keyword in relevant_keywords):
            all_articles.append({
                'title': title,
                'description': description,
                'url': url,
                'source': "NewsAPI",
                'image': image
            })

    seen_titles = set(article['title'] for article in all_articles)
    for article in scraped_articles:
        if article['title'] not in seen_titles:
            all_articles.append({
                'title': article['title'],
                'description': 'æ­¤æ–°èä¾†è‡ªçˆ¬èŸ²ï¼Œç„¡è©³ç´°æè¿°ã€‚',
                'url': article['url'],
                'source': article['source'],
                'image': article['image']
            })

    return all_articles

# æƒ…æ„Ÿåˆ†æ
def analyze_sentiments(articles):
    for article in articles:
        if re.search(r'[\u4e00-\u9fa5]', article['title']):
            s = SnowNLP(article['title'])
            sentiment_score = s.sentiments
        else:
            sentiment_score = 0.5

        if sentiment_score > 0.6:
            article['sentiment'] = 'ğŸ˜Š'
        elif sentiment_score < 0.4:
            article['sentiment'] = 'ğŸ˜¢'
        else:
            article['sentiment'] = 'ğŸ˜'
    return articles

# å„²å­˜ç‚º HTML
def save_to_html(articles, file_name):
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    html_template = f"""
    <!DOCTYPE html>
    <html lang="zh-Hant">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>æ–°èæ•´ç†</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <style>
            .news-card img {{ max-height: 150px; object-fit: cover; }}
            .news-card {{ margin-bottom: 20px; }}
        </style>
    </head>
    <body>
        <header class="text-white p-3 mb-4" style="background-color: rgb(26, 120, 106);">
            <div class="container">
                <h1>ä¿å¥ç›¸é—œå³æ™‚æ–°è</h1>
            </div>
        </header>
        <div class="container">
            <div class="row">
    """

    for article in articles:
        html_template += f"""
            <div class="col-md-4 news-card">
                <div class="card">
                    <img src="{article['image']}" class="card-img-top" alt="æ–°èåœ–ç‰‡">
                    <div class="card-body">
                        <h5 class="card-title"><a href="{article['url']}" target="_blank">{article['title']}</a></h5>
                        <p class="card-text">{article['description']}</p>
                        <p class="text-muted">ä¾†æº: {article['source']}</p>
                        <p class="text-muted">æƒ…æ„Ÿåˆ†æ: {article['sentiment']}</p>
                    </div>
                </div>
            </div>
        """

    html_template += f"""
            </div>
        </div>
        <footer class="bg-light text-center py-3">
            <p>æ›´æ–°æ™‚é–“: {current_time}</p>
        </footer>
    </body>
    </html>
    """

    with open(file_name, 'w', encoding='utf-8') as f:
        f.write(html_template)

# å„²å­˜ç‚º JSON
def save_to_json(articles, file_name):
    with open(file_name, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=4)

# ä¸»ç¨‹åº
fetch_api_articles()
healthnews_articles = scrape_healthnews()
yahoo_articles = scrape_yahoo_health()
all_articles = integrate_articles(api_articles, healthnews_articles + yahoo_articles)
analyzed_articles = analyze_sentiments(all_articles)

save_to_html(analyzed_articles, 'news.html')
save_to_json(analyzed_articles, 'news.json')

print(f"å·²åŒ¯å‡º {len(analyzed_articles)} ç¯‡æ–°èè‡³ news.html å’Œ news.json")