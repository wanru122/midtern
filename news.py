import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
from datetime import datetime, timedelta
import json
from snownlp import SnowNLP
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import time
import re
import logging

logging.basicConfig(filename='error.log', level=logging.ERROR, format='%(asctime)s - %(message)s')  

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
apikey = os.getenv("API_KEY")

# å®šç¾©æ—¥æœŸç¯„åœ
today = datetime.today()
month_ago = today - timedelta(days=30)
default_from_date = month_ago.strftime('%Y-%m-%d')
default_to_date = today.strftime('%Y-%m-%d')

# å®šç¾©ç›¸é—œé—œéµå­—
synonyms = {
    "å¥åº·": ["å¥åº·", "å¥èº«", "å¥åº·ç”Ÿæ´»"],
    "ç–¾ç—…": ["ç–¾ç—…", "ç—…ç—‡", "ç—…æƒ…"],
    "é†«ç™‚": ["é†«ç™‚", "é†«å­¸", "é†«è­·"],
    "é†«ç”Ÿ": ["é†«ç”Ÿ", "é†«å¸«"],
    "é†«é™¢": ["é†«é™¢", "è¨ºæ‰€"],
    "ä¿å¥": ["ä¿å¥", "é¤Šç”Ÿ", "è­·ç†"],
    "å…ç–«": ["å…ç–«", "å…ç–«ç³»çµ±"],
    "ç–«è‹—": ["ç–«è‹—", "æ¥ç¨®", "å…ç–«"],
    "è—¥ç‰©": ["è—¥ç‰©", "è—¥å“", "è—¥æ•ˆ"],
    "æ²»ç™‚": ["æ²»ç™‚", "é†«æ²»", "ç™‚æ³•"],
    "é¤Šç”Ÿ": ["é¤Šç”Ÿ", "ä¿å¥"],
    "ç–«æƒ…": ["ç–«æƒ…", "å‚³æŸ“ç—…", "ç–«ç—…"],
    "å¿ƒç†": ["å¿ƒç†", "å¿ƒç†å¥åº·", "ç²¾ç¥"],
}

relevant_keywords = list(set(word for synonyms_list in synonyms.values() for word in synonyms_list))

# æŸ¥è©¢åƒæ•¸è¨­å®š
query_params = {
    'q': " OR ".join(relevant_keywords),
    'from': default_from_date,
    'to': default_to_date,
}

# ä½¿ç”¨è€…è‡ªå®šç¾©é—œéµå­—
user_query = input(f"è«‹è¼¸å…¥é—œéµå­—ï¼ˆé è¨­: {query_params['q']}ï¼‰ï¼š") or query_params['q']
query_params['q'] = user_query

# APIè«‹æ±‚åƒæ•¸
params = {
    'q': query_params['q'],
    'language': 'zh',
    'from': query_params['from'],
    'to': query_params['to'],
    'apiKey': apikey,
    'sortBy': 'popularity',
    'pageSize': 100,
    'page': 1
}

# ç”¨ä¾†å„²å­˜ API æŠ“å–çš„æ–‡ç« 
api_articles = []

# API æŠ“å–å‡½æ•¸
def fetch_api_articles():
    global api_articles
    for page in range(1, 2):  # åªè«‹æ±‚ç¬¬ 1 é ï¼Œé¿å…è¶…éé™åˆ¶
        params['page'] = page
        try:
            response = requests.get('https://newsapi.org/v2/everything', params=params)
            response.raise_for_status()
            articles = response.json().get('articles', [])
            api_articles.extend(articles)
            if not articles:
                break
        except requests.exceptions.RequestException as e:
            logging.error(f"API ç¬¬ {page} é è«‹æ±‚å¤±æ•—: {e}")
            break

# çˆ¬å–å¥åº·é†«ç™‚ç¶²

def scrape_healthnews():
    articles = []
    try:
        url = "https://www.healthnews.com.tw/"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        for article in soup.find_all('a'):
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link or "javascript:void(0)" in link:
                continue
            if not link.startswith("http"):
                link = url.rstrip("/") + link
            if any(keyword in title for keyword in relevant_keywords):
                articles.append({
                    'title': title,
                    'url': link,
                    'source': "å¥åº·é†«ç™‚ç¶²",
                    'image': "https://via.placeholder.com/300"
                })
    except requests.exceptions.RequestException as e:
        logging.error(f"çˆ¬å–å¥åº·é†«ç™‚ç¶²å¤±æ•—: {e}")
    return articles

# çˆ¬å– Yahoo å¥åº·å°ˆå€

def fetch_yahoo_health_with_selenium():
    url = "https://tw.news.yahoo.com/health"
    options = Options()
    options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼
    options.add_argument("--disable-gpu")
    service = Service("path_to_chromedriver")  

    with webdriver.Chrome(service=service, options=options) as driver:
        driver.get(url)
        time.sleep(2) 


        try:
            if "consent.yahoo" in driver.current_url:
                logging.info("åŒæ„é é¢å‡ºç¾ï¼Œå˜—è©¦é»æ“ŠåŒæ„æŒ‰éˆ•ã€‚")
                consent_button = driver.find_element(By.XPATH, "//button[contains(text(), 'Agree')]")
                consent_button.click()
                time.sleep(2)
        except Exception as e:
            logging.warning(f"æœªæ‰¾åˆ°åŒæ„æŒ‰éˆ•ï¼š{e}")

        return driver.page_source

def scrape_yahoo_health():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
    }
    url = "https://tw.news.yahoo.com/health"
    response = requests.get(url, headers=headers, allow_redirects=True)

    if "consent.yahoo" in response.url:
        logging.info("æª¢æ¸¬åˆ°åŒæ„é é¢ï¼Œåˆ‡æ›è‡³ Selenium æ¨¡æ“¬æ“ä½œã€‚")
        return fetch_yahoo_health_with_selenium()

    return response.text

html_content = scrape_yahoo_health()
print("æˆåŠŸç²å–é é¢å…§å®¹ï¼")

def scrape_yahoo_health():
    articles = []
    try:
        url = "https://tw.news.yahoo.com/health"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, allow_redirects=True)

        if "consent.yahoo.com" in response.url:
            logging.warning("è¢«é‡å®šå‘è‡³åŒæ„é é¢ï¼Œè·³éæ­¤é é¢ã€‚")
            return articles

        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        for article in soup.find_all('a'):
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link:
                continue
            if not link.startswith("http"):
                link = "https://tw.news.yahoo.com" + link
            if any(keyword in title for keyword in relevant_keywords):
                articles.append({
                    'title': title,
                    'url': link,
                    'source': "Yahoo å¥åº·å°ˆå€",
                    'image': "https://via.placeholder.com/300"
                })
    except requests.exceptions.RequestException as e:
        logging.error(f"çˆ¬å– Yahoo å¥åº·å°ˆå€å¤±æ•—: {e}")
    return articles

# æ•´åˆæ–‡ç« 
def integrate_articles(api_articles, scraped_articles):
    all_articles = []

    for article in api_articles:
        title = article.get('title', 'ç„¡æ¨™é¡Œ')
        description = article.get('description', 'ç„¡æè¿°')
        content = article.get('content', '')
        url = article.get('url', '#')
        image = article.get('urlToImage', 'https://via.placeholder.com/300')

        if any(keyword in title or keyword in description or keyword in content for keyword in relevant_keywords):
            all_articles.append({
                'title': title,
                'description': description,
                'url': url,
                'source': "NewsAPI",
                'image': image
            })

    seen_titles = set(article['title'] for article in all_articles)
    for article in scraped_articles:
        if article['title'] not in seen_titles:
            all_articles.append({
                'title': article['title'],
                'description': 'æ­¤æ–°èä¾†è‡ªçˆ¬èŸ²ï¼Œç„¡è©³ç´°æè¿°ã€‚',
                'url': article['url'],
                'source': article['source'],
                'image': article['image']
            })

    return all_articles

# ä½¿ç”¨ SnowNLP é€²è¡Œæƒ…æ„Ÿåˆ†æ
def analyze_sentiments(articles):
    for article in articles:
        if re.search(r'[\u4e00-\u9fa5]', article['title']):
            s = SnowNLP(article['title'])
            sentiment_score = s.sentiments
        else:
            sentiment_score = 0.5

        if sentiment_score > 0.6:
            article['sentiment'] = 'ğŸ˜Š'
        elif sentiment_score < 0.4:
            article['sentiment'] = 'ğŸ˜¢'
        else:
            article['sentiment'] = 'ğŸ˜'
    return articles

# å®šç¾©å‡½æ•¸ä¾†åŠ ç²—é—œéµå­—
def highlight_keywords(text, keywords):
    for keyword in keywords:
        text = re.sub(r'(?i)(' + re.escape(keyword) + r')', r'<b>\1</b>', text)
    return text

# å„²å­˜åˆ°HTMLæª”æ¡ˆ
def save_to_html(articles, file_name):
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    html_template = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>æ–°èæ•´ç†</title>
    <link rel="stylesheet" type="text/css" href="liang.css">
</head>
<body>
    <header>
        <a href="index.html"><h3><img src="image/logo.jpg" class="logo"></h3></a>
        <nav>
            <a href="news.html"><h3>ç›¸é—œæ–°è</h3></a>
            <a href="å¥åº·åˆ†æå·¥å…·/bmi/submit.html"><h3>å¥åº·åˆ†æå·¥å…·</h3></a>
            <a href="cb.html"><h3>Chat Bot</h3></a>
            <a href="test.html"><h3>ç™¾è¬å°å­¸å ‚</h3></a>
            <a href="health.html"><h3>å¥åº·æª¢æŸ¥æé†’</h3></a>
        </nav>
    </header>
    <div class="news-container">
"""

    for article in articles:
        highlighted_title = highlight_keywords(article["title"], relevant_keywords)
        highlighted_description = highlight_keywords(article["description"], relevant_keywords)

        html_template += f"""
        <div class="news-card">
            <h2><a href="{article['url']}" target="_blank">{highlighted_title}</a></h2>
            <p>{highlighted_description}</p>
            <p>æƒ…æ„Ÿåˆ†æ: {article['sentiment']}</p>
        </div>
        """

    html_template += f"""
    </div>
    <a href="#" class="back-to-top">å›åˆ°æœ€ä¸Šé¢</a>
    <footer class='footer2'>
        <p>ä¿å¥å·´æ‹‰æ‹‰. ç‚ºæ‚¨æœå‹™</p>
        <p>æ›´æ–°æ™‚é–“: {current_time}</p>
    </footer>
</body>
</html>
"""

    with open(file_name, 'w+', encoding='utf-8') as f:
        f.write(html_template)

# å„²å­˜ç‚º JSON
def save_to_json(articles, file_name):
    with open(file_name, 'w+', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=4)

# ä¸»ç¨‹åº
fetch_api_articles()
healthnews_articles = scrape_healthnews()
yahoo_articles = scrape_yahoo_health()
all_articles = integrate_articles(api_articles, healthnews_articles + yahoo_articles)
analyzed_articles = analyze_sentiments(all_articles)

save_to_html(analyzed_articles, 'news.html')
save_to_json(analyzed_articles, 'news.json')

print(f"å·²åŒ¯å‡º {len(analyzed_articles)} ç¯‡æ–°èè‡³ news.html å’Œ news.json")