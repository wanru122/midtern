import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
from datetime import datetime, timedelta
import json
from snownlp import SnowNLP
import random
import time
import re

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
apikey = os.getenv("API_KEY")

# å®šç¾©æ—¥æœŸç¯„åœ
today = datetime.today()
month_ago = today - timedelta(days=30)
default_from_date = month_ago.strftime('%Y-%m-%d')
default_to_date = today.strftime('%Y-%m-%d')

# å®šç¾©ç›¸é—œé—œéµå­—
synonyms = {
    "å¥åº·": ["å¥åº·", "å¥èº«", "å¥åº·ç”Ÿæ´»"],
    "ç–¾ç—…": ["ç–¾ç—…", "ç—…ç—‡", "ç—…æƒ…"],
    "é†«ç™‚": ["é†«ç™‚", "é†«å­¸", "é†«è­·"],
    "é†«ç”Ÿ": ["é†«ç”Ÿ", "é†«å¸«"],
    "é†«é™¢": ["é†«é™¢", "è¨ºæ‰€"],
    "ä¿å¥": ["ä¿å¥", "é¤Šç”Ÿ", "è­·ç†"],
    "å…ç–«": ["å…ç–«", "å…ç–«ç³»çµ±"],
    "ç–«è‹—": ["ç–«è‹—", "æ¥ç¨®", "å…ç–«"],
    "è—¥ç‰©": ["è—¥ç‰©", "è—¥å“", "è—¥æ•ˆ"],
    "æ²»ç™‚": ["æ²»ç™‚", "é†«æ²»", "ç™‚æ³•"],
    "é¤Šç”Ÿ": ["é¤Šç”Ÿ", "ä¿å¥"],
    "ç–«æƒ…": ["ç–«æƒ…", "å‚³æŸ“ç—…", "ç–«ç—…"],
    "å¿ƒç†": ["å¿ƒç†", "å¿ƒç†å¥åº·", "ç²¾ç¥"],
}

# æ•´ç†ç›¸é—œçš„é—œéµå­—
relevant_keywords = set()
for keyword, synonym_list in synonyms.items():
    relevant_keywords.update(synonym_list)
relevant_keywords = list(relevant_keywords)

# æŸ¥è©¢åƒæ•¸è¨­å®š
query_params = {
    'q': " OR ".join(relevant_keywords),
    'from': default_from_date,
    'to': default_to_date,
}

# ä½¿ç”¨è€…è‡ªå®šç¾©é—œéµå­—
user_query = input(f"è«‹è¼¸å…¥é—œéµå­—ï¼ˆé è¨­: {query_params['q']}ï¼‰ï¼š") or query_params['q']
query_params['q'] = user_query

# APIè«‹æ±‚åƒæ•¸
params = {
    'q': query_params['q'],
    'language': 'zh',
    'from': query_params['from'],
    'to': query_params['to'],
    'apiKey': apikey,
    'sortBy': 'popularity',
    'pageSize': 100,
    'page': 1
}

# ç”¨ä¾†å„²å­˜APIæŠ“å–çš„æ–‡ç« 
api_articles = []

# é€²è¡Œå¤šé æŠ“å–ï¼Œé€™è£¡åªè«‹æ±‚ç¬¬ 1 é ï¼Œé¿å…è¶…éé™åˆ¶
for page in range(1, 2):  # åªè«‹æ±‚ç¬¬ 1 é 
    params['page'] = page
    try:
        response = requests.get('https://newsapi.org/v2/everything', params=params)
        response.raise_for_status()
        articles = response.json().get('articles', [])
        api_articles.extend(articles)
        if not articles:
            break
    except requests.exceptions.RequestException as e:
        print(f"ç¬¬ {page} é è«‹æ±‚å¤±æ•—: {e}")
        break

# çˆ¬å–æ–‡ç« è³‡æ–™
scraped_articles = []

def scrape_healthnews():
    try:
        url = "https://www.healthnews.com.tw/"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = soup.find_all('a')
        for article in articles:
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link or "javascript:void(0)" in link:
                continue  # éæ¿¾æ‰ç„¡æ•ˆéˆæ¥
            if not link.startswith("http"):
                link = "https://www.healthnews.com.tw" + link
            if any(keyword in title for keyword in relevant_keywords):
                scraped_articles.append({
                    'title': title,
                    'url': link,
                    'source': "å¥åº·é†«ç™‚ç¶²"
                })
    except requests.exceptions.RequestException as e:
        print(f"çˆ¬å–å¥åº·é†«ç™‚ç¶²å¤±æ•—: {e}")

def scrape_yahoo_health():
    try:
        url = "https://tw.news.yahoo.com/health"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, allow_redirects=True)
        
        # æª¢æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°åŒæ„é é¢
        if "consent.yahoo.com" in response.url:
            print("è¢«é‡å®šå‘è‡³åŒæ„é é¢ï¼Œè·³éæ­¤é é¢ã€‚")
            return
        
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = soup.find_all('a')
        for article in articles:
            title = article.text.strip()
            link = article.get('href', '')
            if not title or not link:
                continue
            if not link.startswith("http"):
                link = "https://tw.news.yahoo.com" + link
            if any(keyword in title for keyword in relevant_keywords):
                scraped_articles.append({
                    'title': title,
                    'url': link,
                    'source': "Yahoo å¥åº·å°ˆå€"
                })
    except requests.exceptions.RequestException as e:
        print(f"çˆ¬å– Yahoo å¥åº·å°ˆå€å¤±æ•—: {e}")



scrape_healthnews()
scrape_yahoo_health()

# æ•´åˆAPIå’Œçˆ¬èŸ²æŠ“å–çš„æ–‡ç« 
all_articles = []

for article in api_articles:
    title = article.get('title', 'ç„¡æ¨™é¡Œ')
    description = article.get('description', 'ç„¡æè¿°')
    content = article.get('content', '')
    url = article.get('url', '#')

    if any(keyword in title or keyword in description or keyword in content for keyword in relevant_keywords):
        all_articles.append({
            'title': title,
            'description': description,
            'url': url,
            'source': "NewsAPI"
        })

seen_titles = set([article['title'] for article in all_articles])
for article in scraped_articles:
    if article['title'] not in seen_titles:
        all_articles.append({
            'title': article['title'],
            'description': 'æ­¤æ–°èä¾†è‡ªçˆ¬èŸ²ï¼Œç„¡è©³ç´°æè¿°ã€‚',
            'url': article['url'],
            'source': article['source']
        })

# ä½¿ç”¨ SnowNLP é€²è¡Œæƒ…æ„Ÿåˆ†æ
for article in all_articles:
    s = SnowNLP(article['title'])
    sentiment_score = s.sentiments

    if sentiment_score > 0.6:
        article['sentiment'] = 'ğŸ˜Š'  
    elif sentiment_score < 0.4:
        article['sentiment'] = 'ğŸ˜¢'  
    else:
        article['sentiment'] = 'ğŸ˜'  

time.sleep(random.uniform(1, 3))

# å®šç¾©å‡½æ•¸ä¾†åŠ ç²—é—œéµå­—
def highlight_keywords(text, keywords):
    for keyword in keywords:
        text = re.sub(r'(?i)(' + re.escape(keyword) + r')', r'<b>\1</b>', text)
    return text

# å„²å­˜åˆ°HTMLæª”æ¡ˆ
html_file = 'news.html'
current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
with open(html_file, mode='w+', encoding='utf-8') as file:
    file.write(f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>æ–°èæ•´ç†</title>
    <link rel="stylesheet" type="text/css" href="liang.css">
</head>
<body>
    <header>
        <a href="index.html"><h3><img src="image/logo.jpg" class="logo"></h3></a>
        <nav>
            <a href="news.html"><h3>ç›¸é—œæ–°è</h3></a>
            <a href="å¥åº·åˆ†æå·¥å…·/bmi/submit.html"><h3>å¥åº·åˆ†æå·¥å…·</h3></a>
            <a href="cb.html"><h3>Chat Bot</h3></a>
            <a href="test.html"><h3>ç™¾è¬å°å­¸å ‚</h3></a>
            <a href="health.html"><h3>å¥åº·æª¢æŸ¥æé†’</h3></a>
        </nav>
    </header>
    <div class="news-container">
''')

    for article in all_articles:
        highlighted_title = highlight_keywords(article["title"], relevant_keywords)
        highlighted_description = highlight_keywords(article["description"], relevant_keywords)

        file.write(f'''
        <div class="news-card">
            <h2><a href="{article["url"]}" target="_blank">{highlighted_title}</a></h2>
            <p>{highlighted_description}</p>
            <p>æƒ…æ„Ÿåˆ†æ: {article["sentiment"]}</p>
        </div>
        ''')

    file.write(f'''
    </div>
               <a href="#" class="back-to-top">å›åˆ°æœ€ä¸Šé¢</a>
    <footer class='footer2'>
        <p>ä¿å¥å·´æ‹‰æ‹‰. ç‚ºæ‚¨æœå‹™</p>
        <p>æ›´æ–°æ™‚é–“: {current_time}</p>
    </footer>
</body>
</html>
''')

print(f"\nå·²åŒ¯å‡º {len(all_articles)} ç¯‡æ–°èè‡³ {html_file}")


# å„²å­˜ç‚ºJSON
json_file = 'news.json'
with open(json_file, mode='w+', encoding='utf-8') as file:
    json.dump(all_articles, file, ensure_ascii=False, indent=4)

print(f"\nå·²åŒ¯å‡º {len(all_articles)} ç¯‡æ–°èè‡³ {json_file}")
